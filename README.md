
#  HPC-Net: Hierarchical Prompt Composition for Memory-Efficient Open-World Continual Learning

> **Official PyTorch implementation** of  
> **"Hierarchical Prompt Composition for Memory-Efficient Open-World Continual Learning in Vision-Language Foundation Models"**  




---

##  Why HPC-Net?

Foundation models like **CLIP** excel at zero-shot recognition but **fail catastrophically** when learning new concepts incrementally. Existing methods either:
-  **Forget old knowledge** (fine-tuning),
-  **Scale poorly** (adapter fusion: +24M params),
-  **Treat classes as independent** (ignoring compositionality).

**HPC-Net solves this** by introducing a **three-tier hierarchical prompt architecture** that:
-  Achieves **84.3% average accuracy** (+5.4% over SOTA)
-  Retains **98.4% of zero-shot performance**
-  Uses only **2.1M additional parameters** (11.6Ã— fewer than Adapter Fusion)
-  Enables **open-world continual learning** with **no task boundaries**

---

##  Core Innovations

### 1. **Three-Tier Prompt Hierarchy**
| Tier | Role | Growth |
|------|------|--------|
| **Foundational Prompts (FP)** | Broad semantic primitives (e.g., "round shape", "high contrast") | Fixed (16 prompts) |
| **Compositional Prompts (CP)** | Mid-level visual patterns (e.g., "circular boundary") | Logarithmic |
| **Instance Prompts (IP)** | Category-specific features | Linear (1 per class) |

>  **Sub-linear parameter growth** confirmed empirically (16 â†’ 41 CPs for 100 classes).

### 2. **Semantic Prototype Anchoring (SPA)**
Prevents semantic drift by anchoring prompts to:
- **FP**: Frozen CLIP text embeddings of visual descriptors
- **IP**: Mean image embeddings from a **fixed 2000-sample buffer**

### 3. **Contrastive Prompt Routing (CPR)**
Dynamically selects **top-56 prompts** per input via sparse attention, enabling:
- Efficient inference (8.9 ms/sample)
- Compositional generalization

### 4. **Open-World Continual Learning Score (OWCLS)**
A holistic metric balancing:
- **Plasticity** ($A_t$): Accuracy on seen classes
- **Stability** ($A_{zs}$): Zero-shot retention
- **Forgetting resistance** (BWT)

$$
\text{OWCLS} = 0.5 \cdot A_t + 0.3 \cdot A_{zs} + 0.2 \cdot \text{BWT}
$$

---

##  Results (OWCL Benchmarks)

| Method | Avg. Acc. (%) | Zero-Shot Ret. (%) | BWT | OWCLS |
|--------|---------------|--------------------|-----|-------|
| Finetune | 43.9 | 44.7 | -18.7 | 32.01 |
| iCaRL | 66.4 | 84.8 | -4.3 | 57.88 |
| L2P | 71.7 | 91.7 | -2.9 | 62.59 |
| DualPrompt | 73.8 | 93.8 | -2.1 | 65.02 |
| CODA-Prompt | 76.2 | 95.5 | -1.8 | 67.49 |
| **HPC-Net (Ours)** | **84.3** | **98.4** | **-0.7** | **71.44** |

>  **Statistically significant improvement** ($p < 0.01$) over all baselines.

---

##  Supported Benchmarks

| Dataset | Classes | Tasks | Notes |
|---------|---------|-------|-------|
| **Split-CIFAR100** | 100 | 10 | Standard CL benchmark |
| **Split-ImageNet-R** | 200 | 20 | Artistic renditions of ImageNet |
| **CORe50** | 50 | 10 | Real-world object recognition |
| **MedStream-7k** | 42 | 7 | **New medical benchmark** (7,283 images across 7 modalities) |

> ðŸ’¡ **MedStream-7k**: Built from public, anonymized datasets (ChestX-ray14 + ISIC). Instructions for ethical construction included.

---

## ðŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Run Smoke Test (Validates Gradient Flow & SPA)
```bash
bash scripts/run_smoke.sh
```

### 3. Train on Split-CIFAR100
```bash
python train.py --config configs/default.yaml
```

### 4. Evaluate with Full OWCLS Protocol
```bash
python evaluate.py checkpoints/checkpoint_task_9.pth
```

---

##  Repository Structure

```
hpcnet-paa/
â”œâ”€â”€ configs/               # Training configurations
â”œâ”€â”€ data/                  # Dataset loaders (CIFAR100, ImageNet-R, CORe50, MedStream-7k)
â”œâ”€â”€ models/                # HPC-Net architecture + CLIP wrapper
â”œâ”€â”€ utils/                 # Buffer, metrics, SPA, seeding
â”œâ”€â”€ train.py               # Main training loop with OWCL streaming
â”œâ”€â”€ evaluate.py            # Full OWCLS evaluation (A_t, A_zs, BWT)
â”œâ”€â”€ tests/                 # Unit tests (gradient flow, SPA)
â”œâ”€â”€ scripts/               # Reproducibility scripts
â””â”€â”€ ...
```

---

## ðŸ”’ Ethical Compliance & Reproducibility

- **MedStream-7k**: Constructed from **public, anonymized data** (ChestX-ray14, ISIC)
- **Buffer size**: Fixed at **2000 samples** (aligns with GDPR data minimization & HIPAA minimum necessary standard)
- **No patient identifiers**: All medical data is de-identified
- **Full reproducibility**: Seed-controlled, deterministic training


